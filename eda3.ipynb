{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import utils, models\n",
    "\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/scottbchrist/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/scottbchrist/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/scottbchrist/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to clean text\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude =set(string.punctuation)\n",
    "useless_words = ['would','could','should','le','non','federal','way','hour','lack','make','lot','getting','use','believe','thing']\n",
    "for word in useless_words:\n",
    "    stop.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('data.xlsx')\n",
    "\n",
    "df = data[['AGENCY','COMPONENT','SUB_COMPONENT','GRADELEVEL','SUP_STATUS','Please briefly describe an example of one burdensome administrative task or process which you believe is \"low value\"']]\n",
    "df.columns = ['AGENCY','COMPONENT','SUB_COMPONENT','GRADELEVEL','SUP_STATUS','TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df[df['TEXT'].isnull()==False]\n",
    "full_df = df[df['TEXT'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df[df['COMPONENT'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df[df['GRADELEVEL'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.dropna(subset=['TEXT'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ag = full_df[full_df['AGENCY']=='Department of Agriculture']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ag = df_ag['TEXT'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df.fillna('other')\n",
    "#full_df.replace(pd.isna,'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comps = full_df['COMPONENT'].unique()\n",
    "unique_agenics = full_df['AGENCY'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove emails and newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Remove Emails\n",
    "data_ag = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data_ag]\n",
    "\n",
    "# Remove new line characters\n",
    "data_ag = [re.sub('\\s+', ' ', str(sent)) for sent in data_ag]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data_ag = [re.sub(\"\\'\", \"\", str(sent)) for sent in data_ag]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "#data_ag_words = list(sent_to_words(data_ag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['surveys'], ['am', 'not', 'an', 'administrator', 'some', 'of', 'these', 'questions', 'should', 'have', 'na', 'option']]\n"
    }
   ],
   "source": [
    "print(data_ag_words[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build bigram and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_ag_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_ag_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['administrative', 'work', 'of', 'entering', 'information', 'in', 'multiple', 'spreadsheets', 'or', 'databases', 'there', 'is', 'not', 'enough', 'money', 'to', 'pay', 'for', 'single', 'system', 'that', 'can', 'track', 'what', 'we', 'enter', 'in', 'multiple', 'locations', 'management', 'doesnt', 'seem', 'to', 'understand', 'the', 'need', 'for', 'single', 'system', 'nor', 'the', 'concept', 'of', 'putting', 'money', 'out', 'now', 'that', 'in', 'return', 'will', 'benefit', 'the', 'office', 'in', 'the', 'long', 'run']\n"
    }
   ],
   "source": [
    "print(trigram_mod[bigram_mod[data_ag_words[12]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords, make Bigrams and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_ag_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['survey'], ['administrator', 'question', 'option'], ['system', 'slow', 'have', 'put', 'build', 'horrible', 'internet', 'service', 'agency', 'constantly', 'work', 'system', 'change', 'well']]\n"
    }
   ],
   "source": [
    "print(data_lemmatized[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dictionary and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[(0, 1)], [(1, 1), (2, 1), (3, 1)]]\n"
    }
   ],
   "source": [
    "print(corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moved to utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=20, step=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moved to  calc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {'Department of Agriculture':{\n",
    "'scores':{'total':8,'manager':4,'nonmanager':8\n",
    "}},\n",
    "'Department of Commerce':{\n",
    "'scores':{'total':6,'manager':4,'nonmanager':6\n",
    "}},\n",
    "'Department of Defense':{\n",
    "'scores':{'total':10,'manager':16,'nonmanager':8\n",
    "}},\n",
    "'Department of Education':{\n",
    "'scores':{'total':6,'manager':14,'nonmanager':8\n",
    "}},\n",
    "'Department of Energy':{\n",
    "'scores':{'total':4,'manager':18,'nonmanager':6\n",
    "}},\n",
    "'Department of Health and Human Services':{\n",
    "'scores':{'total':6,'manager':10,'nonmanager':10\n",
    "}},\n",
    "'Department of Homeland Security':{\n",
    "'scores':{'total':6,'manager':6,'nonmanager':6\n",
    "}},\n",
    "'Department of Housing and Urban Development':{\n",
    "'scores':{'total':4,'manager':10,'nonmanager':4\n",
    "}},\n",
    "'Department of Justice':{\n",
    "'scores':{'total':8,'manager':12,'nonmanager':4\n",
    "}},\n",
    "'Department of Labor':{\n",
    "'scores':{'total':8,'manager':14,'nonmanager':6\n",
    "}},\n",
    "'Department of State':{\n",
    "'scores':{'total':14,'manager':6,'nonmanager':6\n",
    "}},\n",
    "'Department of the Interior':{\n",
    "'scores':{'total':10,'manager':14,'nonmanager':4\n",
    "}},\n",
    "'Department of the Treasury':{\n",
    "'scores':{'total':8,'manager':10,'nonmanager':8\n",
    "}},\n",
    "'Department of Transportation':{\n",
    "'scores':{'total':6,'manager':16,'nonmanager':6\n",
    "}},\n",
    "'Department of Veterans Affairs':{\n",
    "'scores':{'total':8,'manager':12,'nonmanager':6\n",
    "}},\n",
    "'Environmental Protection Agency':{\n",
    "'scores':{'total':14,'manager':12,'nonmanager':10\n",
    "}},\n",
    "'General Services Administration':{\n",
    "'scores':{'total':18,'manager':8,'nonmanager':6\n",
    "}},\n",
    "'National Aeronautics and Space Administration':{\n",
    "'scores':{'total':4,'manager':18,'nonmanager':6\n",
    "}},\n",
    "'National Science Foundation':{\n",
    "'scores':{'total':4,'manager':16,'nonmanager':12\n",
    "}},\n",
    "'Nuclear Regulatory Commission':{\n",
    "'scores':{'total':6,'manager':16,'nonmanager':6\n",
    "}},\n",
    "'Office of Personnel Management':{\n",
    "'scores':{'total':6,'manager':12,'nonmanager':6\n",
    "}},\n",
    "'Social Security Administration':{\n",
    "'scores':{'total':10,'manager':6,'nonmanager':6\n",
    "}},\n",
    "'Small Business Administration':{\n",
    "'scores':{'total':6,'manager':16,'nonmanager':6\n",
    "}},\n",
    "'U.S. Agency for International Development':{\n",
    "'scores':{'total':4,'manager':12,'nonmanager':8\n",
    "}\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict_keys(['Department of Agriculture', 'Department of Commerce', 'Department of Defense', 'Department of Education', 'Department of Energy', 'Department of Health and Human Services', 'Department of Homeland Security', 'Department of Housing and Urban Development', 'Department of Justice', 'Department of Labor', 'Department of State', 'Department of the Interior', 'Department of the Treasury', 'Department of Transportation', 'Department of Veterans Affairs', 'Environmental Protection Agency', 'General Services Administration', 'National Aeronautics and Space Administration', 'National Science Foundation', 'Nuclear Regulatory Commission', 'Office of Personnel Management', 'Social Security Administration', 'Small Business Administration', 'U.S. Agency for International Development'])"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "scores.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Topics for agency level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_agencies(df,topic_num):\n",
    "    data_2 = df['TEXT'].values.tolist()\n",
    "\n",
    "    # Remove Emails\n",
    "    data_2 = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data_2]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data_2 = [re.sub('\\s+', ' ', str(sent)) for sent in data_2]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data_2 = [re.sub(\"\\'\", \"\", str(sent)) for sent in data_2]\n",
    "\n",
    "    data_words_2 = list(sent_to_words(data_2))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram2 = gensim.models.Phrases(data_words_2, min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "    trigram2 = gensim.models.Phrases(bigram2[data_words_2], threshold=50)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod2 = gensim.models.phrases.Phraser(bigram2)\n",
    "    trigram_mod2= gensim.models.phrases.Phraser(trigram2)\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops2 = remove_stopwords(data_words_2)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams2 = make_bigrams(data_words_nostops2,bigram_mod2)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized2 = lemmatization(data_words_bigrams2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word2 = corpora.Dictionary(data_lemmatized2)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts2 = data_lemmatized2\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus2 = [id2word2.doc2bow(text) for text in texts2]\n",
    "\n",
    "    #model_list, coherence_values = compute_coherence_values(dictionary=id2word2, corpus=corpus2, texts=data_lemmatized2, start=2, limit=20, step=2)\n",
    "\n",
    "    #print(coherence_values)\n",
    "    #max_coherence_score = max(coherence_values)\n",
    "    #best_num_loc = coherence_values.index(max_coherence_score)\n",
    "    #best_topic_num = (coherence_values.index(max_coherence_score) + 1) *2\n",
    "    #print (best_topic_num)\n",
    "\n",
    "    #best_model = model_list[best_num_loc]\n",
    "\n",
    "    #model_topics = best_model.show_topics(formatted=False)\n",
    "\n",
    "    model_2 = gensim.models.ldamodel.LdaModel( corpus=corpus2, num_topics=topic_num, id2word=id2word2)\n",
    "\n",
    "    #pprint(best_model.print_topics(num_words=8))\n",
    "    #pprint(model_2.print_topics(num_words=8))\n",
    "\n",
    "    return model_2.show_topics(num_words=8,formatted=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(df):\n",
    "\n",
    "\n",
    "    data_2 = df['TEXT'].values.tolist()\n",
    "    if len(data_2)==0:\n",
    "        return 'empty'\n",
    "\n",
    "    # Remove Emails\n",
    "    data_2 = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data_2]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data_2 = [re.sub('\\s+', ' ', str(sent)) for sent in data_2]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data_2 = [re.sub(\"\\'\", \"\", str(sent)) for sent in data_2]\n",
    "\n",
    "    data_words_2 = list(sent_to_words(data_2))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram2 = gensim.models.Phrases(data_words_2, min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "    trigram2 = gensim.models.Phrases(bigram2[data_words_2], threshold=50)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod2 = gensim.models.phrases.Phraser(bigram2)\n",
    "    trigram_mod2= gensim.models.phrases.Phraser(trigram2)\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops2 = remove_stopwords(data_words_2)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams2 = make_bigrams(data_words_nostops2,bigram_mod2)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized2 = lemmatization(data_words_bigrams2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word2 = corpora.Dictionary(data_lemmatized2)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts2 = data_lemmatized2\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus2 = [id2word2.doc2bow(text) for text in texts2]\n",
    "    try:\n",
    "        model_list, coherence_values = compute_coherence_values(dictionary=id2word2, corpus=corpus2, texts=data_lemmatized2, start=2, limit=20,\\\n",
    "                   step=2,id2word=id2word2)\n",
    "    except ValueError:\n",
    "        return 'no data'\n",
    "\n",
    "    #print(coherence_values)\n",
    "    max_coherence_score = max(coherence_values)\n",
    "    best_num_loc = coherence_values.index(max_coherence_score)\n",
    "    best_topic_num = (coherence_values.index(max_coherence_score) + 1) *2\n",
    "    #print (best_topic_num)\n",
    "\n",
    "    #best_model = model_list[best_num_loc]\n",
    "\n",
    "    #model_topics = best_model.show_topics(formatted=False)\n",
    "\n",
    "    model_2 = gensim.models.ldamodel.LdaModel( corpus=corpus2, num_topics=best_topic_num, id2word=id2word2)\n",
    "\n",
    "    #pprint(best_model.print_topics(num_words=8))\n",
    "    #pprint(model_2.print_topics(num_words=8))\n",
    "\n",
    "    return model_2.show_topics(num_words=8,formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_dict_agency={}\n",
    "key_values = ['total', 'manager', 'nonmanager']\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    topic_dict_agency[agency] = get_topics_agencies(full_df[full_df['AGENCY']==agency],scores.get(agency).get('scores').get('total'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in topic_dict_agency.items() ])).T\n",
    "agency_df.to_csv('Agency_Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict_agency_mang={}\n",
    "key_values = ['total', 'manager', 'nonmanager']\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    topic_dict_agency_mang[agency] = get_topics_agencies(full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==1)],scores.get(agency).get('scores').get('manager'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_df_mang = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in topic_dict_agency_mang.items() ])).T\n",
    "agency_df_mang.to_csv('Agency_Topics_Senior_Manager.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict_agency_nonmang={}\n",
    "key_values = ['total', 'manager', 'nonmanager']\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    topic_dict_agency_nonmang[agency] = get_topics_agencies(full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==0)],scores.get(agency).get('scores').get('nonmanager'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_df_nonmang = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in topic_dict_agency_nonmang.items() ])).T\n",
    "agency_df_nonmang.to_csv('Agency_Non_Manager_Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_grades = full_df['GRADELEVEL'].unique()\n",
    "grade_topic_dict = {}\n",
    "for grade in unique_grades:\n",
    "    grade_topic_dict[grade] = get_topics(full_df[full_df['GRADELEVEL']==grade])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in grade_topic_dict.items() ])).T\n",
    "gs_df.to_csv('GS_Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_comps_topics_mang = {}\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    temp_dict = {}\n",
    "\n",
    "    for comps in full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==1)]['COMPONENT'].unique():\n",
    "        temp_dict[comps] = get_topics(full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==1)&(full_df['COMPONENT']==comps)])\n",
    "    \n",
    "    unique_comps_topics_mang[agency] = temp_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_mang_df = pd.DataFrame([(k,k1,pd.Series(v1)) for k,v in unique_comps_topics_mang.items() for k1,v1 in v.items() ],columns=['Agency','Component','topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_list = ['Agency','Component','topics','topic 0','topic 1','topic 2', 'topic 3','topic 4','topic 5','topic 6','topic 7', 'topic 8','topic 9']\n",
    "comps_mang_df = comps_mang_df.reindex(columns = headers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(comps_mang_df)):\n",
    "    for t in range((10)):\n",
    "        try:\n",
    "            comps_mang_df['topic '+str(t)].iloc[i] = comps_mang_df['topics'].iloc[i][t][1]\n",
    "        except KeyError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_mang_df.to_csv('Component_SR_Manager.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comps_topics_nonmang = {}\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    temp_dict = {}\n",
    "\n",
    "    for comps in full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==0)]['COMPONENT'].unique():\n",
    "        temp_dict[comps] = get_topics(full_df[(full_df['AGENCY']==agency) & (full_df['SUP_STATUS']==0)&(full_df['COMPONENT']==comps)])\n",
    "    \n",
    "    unique_comps_topics_nonmang[agency] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_nonmang_df = pd.DataFrame([(k,k1,pd.Series(v1)) for k,v in unique_comps_topics_nonmang.items() for k1,v1 in v.items() ],columns=['Agency','Component','topics'])\n",
    "headers_list = ['Agency','Component','topics','topic 0','topic 1','topic 2', 'topic 3','topic 4','topic 5','topic 6','topic 7', 'topic 8','topic 9']\n",
    "comps_nonmang_df = comps_nonmang_df.reindex(columns = headers_list)\n",
    "for i in range(len(comps_nonmang_df)):\n",
    "    for t in range((10)):\n",
    "        try:\n",
    "            comps_nonmang_df['topic '+str(t)].iloc[i] = comps_nonmang_df['topics'].iloc[i][t][1]\n",
    "        except KeyError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_nonmang_df.to_csv('Component_Non_Manager.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comps_topics = {}\n",
    "\n",
    "for agency in unique_agenics:\n",
    "    temp_dict = {}\n",
    "\n",
    "    for comps in full_df[(full_df['AGENCY']==agency)]['COMPONENT'].unique():\n",
    "        temp_dict[comps] = get_topics(full_df[(full_df['AGENCY']==agency)&(full_df['COMPONENT']==comps)])\n",
    "    \n",
    "    unique_comps_topics[agency] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_df = pd.DataFrame([(k,k1,pd.Series(v1)) for k,v in unique_comps_topics.items() for k1,v1 in v.items() ],columns=['Agency','Component','topics'])\n",
    "headers_list = ['Agency','Component','topics','topic 0','topic 1','topic 2', 'topic 3','topic 4','topic 5','topic 6','topic 7', 'topic 8','topic 9']\n",
    "comps_df = comps_df.reindex(columns = headers_list)\n",
    "for i in range(len(comps_df)):\n",
    "    for t in range((10)):\n",
    "        try:\n",
    "            comps_df['topic '+str(t)].iloc[i] = comps_df['topics'].iloc[i][t][1]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_df.to_csv('Component_Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open('GS_Topics.csv','r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_text = f.read()\n",
    "list1=re.sub(r\"[^a-zA-Z]+\", ' ',f_text)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = open('Agency_Topics.csv','r')\n",
    "g_text = g.read()\n",
    "listg=re.sub(r\"[^a-zA-Z]+\", ' ',g_text)\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = open('Component_SR_manager.csv','r')\n",
    "h_text = h.read()\n",
    "listh=re.sub(r\"[^a-zA-Z]+\", ' ',h_text)\n",
    "h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = open('Component_Topics.csv','r')\n",
    "j_text = j.read()\n",
    "listj=re.sub(r\"[^a-zA-Z]+\", ' ',j_text)\n",
    "j.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = open('Component_Topics.csv','r')\n",
    "k_text = k.read()\n",
    "listk=re.sub(r\"[^a-zA-Z]+\", ' ',k_text)\n",
    "k.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = open('Agency_Topics_Senior_Manager.csv','r')\n",
    "l_text = l.read()\n",
    "listl=re.sub(r\"[^a-zA-Z]+\", ' ',l_text)\n",
    "l.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word = listl+listk+listj+listh+listg+list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open('words.txt',\"w\")\n",
    "text_file.write(new_word)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36664bittextenvvirtualenv3aa4b956ac48498ea5e13bc29009339e",
   "display_name": "Python 3.6.6 64-bit ('text_env': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}